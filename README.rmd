
---

  output:
  md_document:
    variant: markdown_github
    
---

# Purpose

The purpose of this README is to give the solutions to the Financial Econometrics 2021 exam written on 04 December 2021. There are also separate Texevier folders for each questions. 

```{r}
rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

library(knitr)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(error = FALSE)

pacman::p_install_gh("Nicktz/fmxdat")
pacman::p_load(xtable)
library(dplyr)
library(rportfolios)
library(PerformanceAnalytics)
library(rmsfuns)
library(devtools)
if (!require(FactoMineR)) install.packages("FactoMineR")
if (!require(factoextra)) install.packages("factoextra")
library(factoextra)
library(FactoMineR)
pacman::p_load("tidyr","lubridate", "readr", "PerformanceAnalytics", "ggplot2")
SA_bonds <- read_rds("data/SA_Bonds.rds")
BE_Infl <- read_rds("data/BE_Infl.rds")
bonds_2y <- read_rds("data/bonds_2y.rds")
bonds_10y <- read_rds("data/bonds_10y.rds")
usdzar <- read_rds("data/usdzar.rds")
ZA_Infl <- read_rds("data/ZA_Infl.rds")
IV <- read_rds("data/IV.rds")
T40 <- read_rds("data/T40.rds")
RebDays <- read_rds("data/Rebalance_days.rds")
msci <- read_rds("data/msci.rds")
bonds <- read_rds("data/bonds_10y.rds")
comms <- read_rds("data/comms.rds")
cncy <- read_rds("data/currencies.rds")
cncy_Carry <- read_rds("data/cncy_Carry.rds")
cncy_value <- read_rds("data/cncy_value.rds")
cncyIV <- read_rds("data/cncyIV.rds")
bbdxy <- read_rds("data/bbdxy.rds")
msci <- read_rds("data/msci.rds")
bonds <- read_rds("data/bonds_10y.rds")
comms <- read_rds("data/comms.rds")
pacman::p_load("MTS", "robustbase")
pacman::p_load("tidyverse", "devtools", "rugarch", "rmgarch", 
    "forecast", "tbl2xts", "lubridate", 
    "ggthemes")
install_github("vqv/ggbiplot")
library(ggbiplot)
```


# Question 1 

Question 1 requires an analysis of South Africa's Yield Spreads and local mid to longer dated yields bonds. There have been claims that the bond yields are the highest they have been in decades. The plot below confirms this. As seen in the graph, the lowest spread between the local bond market was during the 2007/2008 financial crises, afterwhich it dramatically increases in 2010. The dramatic increase was short lived, with the yield spread increasing slowly between 2011 and 2019. In 2020, the yield spread between the 10 Year and 2 year, as well as the yield spread between the 10 year and 3 month local bonds increased dramatically. The yield spread between the 2 year and 3 month local bonds also experienced an increase, however, this increase reached levels similar to those experienced in 2000/2001. 

In increase in the yield spread can be explained by the uncertainty in the markets during the still on-going COVID-19 pandemic. Investors rushed to raise liquidity amid the uncertainty and panic, particularly those regarded as having a hint of risk. South African government bonds did not escape the effect of the pandemic, with significant sales of the asset class by foreigners, causing yields to spike and prices to move lower. 

```{r}
## SA_Bonds 
plot2 <- SA_bonds %>% 
  mutate("10 Year and 3 Month"=SA_bonds$ZA_10Yr - SA_bonds$SA_3M, "2 Year and 3 Month" = SA_bonds$ZA_2Yr - SA_bonds$SA_3M, "10 Year and 2 Year"= SA_bonds$ZA_10Yr-SA_bonds$ZA_2Yr) %>% select(date, "10 Year and 3 Month", "2 Year and 3 Month", "10 Year and 2 Year") %>% 
  gather(yield_spread, points, -date) %>% ggplot() + 
    geom_line(aes(x=date, y=points, color=yield_spread)) +
    theme_bw() +  
    labs(x = "Dates", y = "Spread", title = "Yeild Spread in Local Bond Market", subtitle = "Comparing Yeild Spread Between 3m, 2Y and 10Y bonds", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Yield Spread Between:"))

plot2
```


This effect felt are not isolated to the South African bond market, with all emerging market local currency debt selling off in unison, as will be shown in the graphs below. This graph follows a similar pattern as the local bonds yield spread. 

```{r}
plot3 <- left_join(SA_bonds %>% select(date, ZA_10Yr), bonds_10y %>% group_by(bonds_10y$Name) %>% filter(Name=="US_10Yr") %>%
                     ungroup() %>% 
                     arrange(date) %>% 
                     filter(date >= lubridate::ymd(19991206)) , by= "date") %>%
  select(-c("bonds_10y$Name", "Name")) %>% mutate( spread = ZA_10Yr - Bond_10Yr) %>% 
  ggplot() + geom_line(aes(x=date, y=spread), color="purple") +
 theme_bw() +  
    labs(x = "Dates", y = "Spread", title = "Yield Spread Between US and SA 10 Year Bonds", subtitle = "", caption = "Note:\nOwn Calculations")

plot3
```

## USD/ZAR level 

The spreads of the 10-year bond yields can be used to gauge currencies. The general rule is that when the yield spread widens in favor of a certain currency, that currency will appreciate against other currencies. 

Yield spreads are often seen as driven by monetary policy. Differences in the exchange rate will drastically affect the expected yield. The graph below shows that this is true for longer term bonds, however, there is more volatility is shorter term bonds implying that the exchange rate is not that good of a predictor and that monetary policy changes will drastically affect the yield. 

```{r}
# Plotting the USD\ZAR exchange rate

plot1 <- usdzar %>% ggplot()+ geom_line(aes(x=date, y=Price), color="blue") +
theme_bw() +  labs(x = "", y = "Prices (ZAR)", title = "USD/ZAR Exchange Rate", subtitle = "Data given in SA currency ", caption = "Note:\nOwn Calculations")

plot1

``` 


## Different Yeild Spreads

Comparing the 10 year and 2 year bond is important, so that investor can determine whether a bond is fairly priced, cheap or expensive. It is a sign of the risk premium for investing in one investment over another. 

```{r}
#Joining datasets, making new columns and and plotting the spread

left_join(SA_bonds %>% select(date, ZA_10Yr), bonds_10y %>% tbl_xts(.,cols_to_xts= "Bond_10Yr", spread_by ="Name") %>% xts_tbl() %>% filter(date >= lubridate::ymd(19991206)) %>% select(date, AUS_10Yr, Canada_10Yr, EURO_10Yr,UK_10Yr) %>% arrange(date), by= "date") %>% 
  mutate( ZA_AUS = ZA_10Yr - AUS_10Yr, ZA_Canada = ZA_10Yr -Canada_10Yr, ZA_EURO = ZA_10Yr - EURO_10Yr, ZA_UK = ZA_10Yr - UK_10Yr) %>% 
  select(date, ZA_AUS,ZA_Canada, ZA_EURO, ZA_UK) %>% gather(country, spread, -date) %>%
 ggplot() + geom_line(aes(x=date, y=spread, color=country)) +
 theme_bw() +  
    labs(x = "Dates", y = "Spread", title = "Yeild Spread Between Different Countries and SA 10 Year Bonds", subtitle = "Countries include United Kingdom, Europe, Cananda and Australia", caption = "Note:\nOwn Calculations")


```


```{r}
# Similar process as the graph but doing it for the 2 year now.

left_join(SA_bonds %>% select(date, ZA_2Yr), bonds_2y %>% tbl_xts(.,cols_to_xts= "Bond_2Yr", spread_by ="Name") %>% xts_tbl() %>% filter(date >= lubridate::ymd(19991206)) %>% select(date, AUS_2yr, Canada_2yr, EURO_2yr,UK_2yr, US_2yr) %>% arrange(date), by= "date") %>% 
  mutate( ZA_AUS = ZA_2Yr - AUS_2yr, ZA_Canada = ZA_2Yr -Canada_2yr, ZA_EURO = ZA_2Yr - EURO_2yr, ZA_UK = ZA_2Yr - UK_2yr, ZA_US = ZA_2Yr - US_2yr) %>% 
  select(date, ZA_AUS,ZA_Canada, ZA_EURO, ZA_UK, ZA_US) %>% 
  gather(country, spread, -date) %>%
 ggplot() + geom_line(aes(x=date, y=spread, color=country)) +
 theme_bw() +  
    labs(x = "Dates", y = "Spread", title = "Yeild Spread Between Different Countries and SA 2 Year Bonds", subtitle = "Countries include United Kingdom, Europe, Cananda, Australia, United States", caption = "Note:\nOwn Calculations")


```
## Inflation 
In principle, the yield spread contains information on expectations of future inflation that can be extracted as an estimate of them. Studies of US and other markets have demonstrated that the yield curve does contain information on expected and actual future levels of gross domestic product (GDP) growth, and can be a lead indicator of business cycle downturns. 

Although many studies indicate the inflation-predicting power of the yield curve in developed countriesâ€™ markets, little is known of the relationship in emerging economies. The graph below mostly supports the notion that the yield spread contains valuable information about expected inflation, where price refers to the inflation rate. However, it is important to notice that inflation has much higher volatility then yield spread. The graph below shows that the higher the current rate of inflation, the higher the yields will rise across the yield curve, as investors will demand this higher yield to compensate for inflation risk



```{r}
data <- left_join(SA_bonds %>% select(date, ZA_10Yr), bonds_10y %>% group_by(bonds_10y$Name) %>% filter(Name=="US_10Yr") %>% ungroup() %>% arrange(date) %>% filter(date >= lubridate::ymd(19991206)) , by= "date") %>% 
  select(-c("bonds_10y$Name", "Name")) %>% 
  mutate( spread = ZA_10Yr - Bond_10Yr) %>% 
  select(date, spread)

left_join(ZA_Infl %>%  filter(date >= lubridate::ymd(19991206)) %>% select(-Name) , data, by = "date") %>% na.omit() %>% gather(Data, Values, -date) %>% 
  ggplot() + geom_line(aes(x=date, y=Values, color=Data)) +
 theme_bw() +  
    labs(x = "Dates", y = "", title = "Comparing Inflation and Yield Spread", subtitle = "Yeild Spread is between US and ZAR 10 year bond", caption = "Note:\nOwn Calculations")
```


# Question 2 

## Portfolio Return 

The porfolio returns for large and mid caps are shown below. Because we are analyzing the Top 40, there are no small caps, and if there are, they make up  such a small percentage of the portfolio that it makes no sense to analyze them. 

The graph for Mid caps return saw very constant returns up until mid year, 2017. Afterwhich it experienced a slight increase followed by a drastic decrease. This would be an indication of an negative, al though constant investment. Investors that are looking for a high return is unlikely to experience this.  The SWIX outperforms the ALSI, which is a preference for when the market is constant/slight increase. However, during negative volatility, the SWIX performs worse. It is important to remember that mid-cap companies can turn into large-cap companies, which also make them attractive to investors. However, you will need to spot the diamond in the rough. 
```{r}
data1 <- read_rds("data/T40.rds") %>% filter(Index_Name=="Mid_Caps") %>% 
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    arrange(date) %>% 
    mutate(year = as.numeric(format(date, format = "%Y%m"))) %>%
    group_by(year, Tickers) %>%
    filter(date==last(date)) %>%
    ungroup 
    
df_Portf_J400<- 
    data1 %>% select(date,Return, J400) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J400, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)

df_Portf_J200 <- 
    data1 %>% select(date,Return, J200) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J200, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)
      
## CULMULATIVE 
Cum_J400 <- df_Portf_J400 %>% mutate(cumreturn_400 = (cumprod(1 + 
    PortfolioReturn))) %>% # Start at 1
mutate(cumreturn_400 = cumreturn_400/first(cumreturn_400)) %>% 
    select(-PortfolioReturn)

Cum_J200 <- df_Portf_J200 %>% mutate(cumreturn_200 = (cumprod(1 + 
    PortfolioReturn))) %>% mutate(cumreturn_200 = cumreturn_200/first(cumreturn_200)) %>% select(-PortfolioReturn)

Cum_Comp <- left_join(Cum_J400, Cum_J200, by = "date") %>% gather(Type, 
    Value, -date)

# Now let's plot the wealth index (if you invested R100 in
# each) of the two portfolios::

Cum_Comp %>% group_by(Type) %>% ggplot() + geom_line(aes(date, 
    Value, color = Type)) + theme_bw() +theme_bw() +  
    labs(x = "Dates", y = "Return", title = "Portfolio Cumulative Returns for Mid Caps", subtitle = "Taking different weight structures into account", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Portfolios:"))
```
The portfolio cumulative returns for Large Caps experience much more volatility then Mid Caps. Large Caps are filled with companies that are dominate in their industry, and while they are still affect by volatility, they have more of a chance to bounce back. They hold themselves well in times of recession or during any other negative event. Besides, they will usually have been functioning for decades and have good reputations. If you want to invest in a companies stocks by taking less risk, then large-cap stocks are a good option. These stocks are less volatile in comparison to mid-cap. The lower volatility makes them less risky.

As seen in th graph below, Large caps present a higher return, but also experienced a downturn with the depletion of economic events in 2020. 

```{r}
data1 <- read_rds("data/T40.rds") %>% filter(Index_Name=="Large_Caps") %>% 
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    arrange(date) %>% 
    mutate(year = as.numeric(format(date, format = "%Y%m"))) %>%
    group_by(year, Tickers) %>%
    filter(date==last(date)) %>%
    ungroup 
    
df_Portf_J400<- 
    data1 %>% select(date,Return, J400) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J400, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)

df_Portf_J200 <- 
    data1 %>% select(date,Return, J200) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J200, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)
      
## CULMULATIVE 
Cum_J400 <- df_Portf_J400 %>% mutate(cumreturn_400 = (cumprod(1 + 
    PortfolioReturn))) %>% # Start at 1
mutate(cumreturn_400 = cumreturn_400/first(cumreturn_400)) %>% 
    select(-PortfolioReturn)

Cum_J200 <- df_Portf_J200 %>% mutate(cumreturn_200 = (cumprod(1 + 
    PortfolioReturn))) %>% mutate(cumreturn_200 = cumreturn_200/first(cumreturn_200)) %>% select(-PortfolioReturn)

Cum_Comp <- left_join(Cum_J400, Cum_J200, by = "date") %>% gather(Type, 
    Value, -date)

# Now let's plot the wealth index (if you invested R100 in
# each) of the two portfolios::

Cum_Comp %>% group_by(Type) %>% ggplot() + geom_line(aes(date, 
    Value, color = Type)) + theme_bw() +theme_bw() +  
    labs(x = "Dates", y = "Return", title = "Portfolio Cumulative Returns for Large Caps", subtitle = "Taking different weight structures into account", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Portfolios:"))
```
## Sector Analysis

The industrial sector has preformed relatively well compared to the financial sector (discussed below). The sector has been on an increasing trend with the COVID-19 pandemic resulting in slowdown and negative growth. 


```{r}
data1 <- T40 %>% filter(Sector=="Industrials") %>% 
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    arrange(date) %>% 
    mutate(year = as.numeric(format(date, format = "%Y%m"))) %>%
    group_by(year, Tickers) %>%
    filter(date==last(date)) %>%
    ungroup 
    
df_Portf_J400<- 
    data1 %>% select(date,Return, J400) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J400, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)

df_Portf_J200 <- 
    data1 %>% select(date,Return, J200) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J200, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)
      
## CULMULATIVE 
Cum_J400 <- df_Portf_J400 %>% mutate(cumreturn_400 = (cumprod(1 + 
    PortfolioReturn))) %>% # Start at 1
mutate(cumreturn_400 = cumreturn_400/first(cumreturn_400)) %>% 
    select(-PortfolioReturn)

Cum_J200 <- df_Portf_J200 %>% mutate(cumreturn_200 = (cumprod(1 + 
    PortfolioReturn))) %>% mutate(cumreturn_200 = cumreturn_200/first(cumreturn_200)) %>% select(-PortfolioReturn)

Cum_Comp <- left_join(Cum_J400, Cum_J200, by = "date") %>% gather(Type, 
    Value, -date)

# Now let's plot the wealth index (if you invested R100 in
# each) of the two portfolios::

Cum_Comp %>% group_by(Type) %>% ggplot() + geom_line(aes(date, 
    Value, color = Type)) + theme_bw() +theme_bw() +  
    labs(x = "Dates", y = "Return", title = "Portfolio Cumulative Returns for Industrial", subtitle = "Taking different weight structures into account", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Portfolios:"))
```
The graph below for the financial structure looks very similar to that of the large-caps (which was discussed above). This indicates that many of the financial sector companies that are being analyzed are large-cap companies. This implies that even though a sharp decrease can be seen, these companies have a high probability to bounce back. 

There are have been some aggressive monetary and fiscal policy responses to the COVID-19 pandemic, which would contribute towards stabilizing the market and ensuring the industry starts growing again. 
```{r}
data1 <- T40 %>% filter(Sector=="Financials") %>% 
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    arrange(date) %>% 
    mutate(year = as.numeric(format(date, format = "%Y%m"))) %>%
    group_by(year, Tickers) %>%
    filter(date==last(date)) %>%
    ungroup 
    
df_Portf_J400<- 
    data1 %>% select(date,Return, J400) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J400, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)

df_Portf_J200 <- 
    data1 %>% select(date,Return, J200) %>% group_by(date) %>% summarise(PortfolioReturn = sum(Return*J200, na.rm =TRUE)) %>% 
      filter(PortfolioReturn != 0)
      
## CULMULATIVE 
Cum_J400 <- df_Portf_J400 %>% mutate(cumreturn_400 = (cumprod(1 + 
    PortfolioReturn))) %>% # Start at 1
mutate(cumreturn_400 = cumreturn_400/first(cumreturn_400)) %>% 
    select(-PortfolioReturn)

Cum_J200 <- df_Portf_J200 %>% mutate(cumreturn_200 = (cumprod(1 + 
    PortfolioReturn))) %>% mutate(cumreturn_200 = cumreturn_200/first(cumreturn_200)) %>% select(-PortfolioReturn)

Cum_Comp <- left_join(Cum_J400, Cum_J200, by = "date") %>% gather(Type, 
    Value, -date)

# Now let's plot the wealth index (if you invested R100 in
# each) of the two portfolios::

Cum_Comp %>% group_by(Type) %>% ggplot() + geom_line(aes(date, 
    Value, color = Type)) + theme_bw() +theme_bw() +  
    labs(x = "Dates", y = "Return", title = "Portfolio Cumulative Returns for Financial Sector", subtitle = "Taking different weight structures into account", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Portfolios:"))
```

## Staritication 

I use the only the index from 2008 onward since this is the only data that is available.  Stratification allows you to isolate the periods of high volatility and low volatility.There are slightly more periods of high volatility than low volatility.This could be an indication to steep negative or positive effects. As the top40 is relatively stable, investors should not panic in times of high volatility, but rather hold stock to ensure growth.  
```{r}

data2 <- T40  %>% 
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    arrange(date) %>% 
    mutate(year = as.numeric(format(date, format = "%Y%m"))) %>%
  select(date, Tickers, Return)

 data2 <- data2 %>%
   group_by(Tickers) %>% 
    mutate(Top = quantile(Return, 0.99), Bot = quantile(Return, 0.01)) %>% 
    mutate(Return = ifelse(Return > Top, Top,ifelse(Return < Bot, Bot, Return))) %>% ungroup() %>% mutate(YearMonth= format(date, "%Y%B"))
 
 ZAR <- usdzar %>% filter(date > ymd(20080101)) %>% select(-Name)
 
 ZARSD <- ZAR %>% 
    mutate(YearMonth = format(date, "%Y%B")) %>% 
    group_by(YearMonth) %>% summarise(SD = sd(Price)*sqrt(252)) %>% 
    mutate(TopQtile = quantile(SD, 0.8), BotQtile = quantile(SD, 0.2))

 Hi_Vol <- ZARSD %>% filter(SD > TopQtile) %>% pull(YearMonth)
 Low_Vol <- ZARSD %>% filter(SD < BotQtile) %>% pull(YearMonth)

Perf_comparisons <- function(data2, YMs, Alias){
    Unconditional_SD <- 
        data2 %>% 
       group_by(Tickers) %>% 
        mutate(Full_SD = sd(Return) * sqrt(252)) %>% 
        filter(YearMonth %in% YMs) %>% 
        summarise(SD = sd(Return) * sqrt(252), across(.cols = starts_with("Full"), .fns = max)) %>% 
        arrange(desc(SD)) %>% mutate(Period = Alias) %>% 
        group_by(Tickers) %>% 
        mutate(Ratio = SD / Full_SD)
     Unconditional_SD
    
}

perf_hi <- Perf_comparisons(data2, YMs = Hi_Vol, Alias = "High_Vol")
perf_lo <- Perf_comparisons(data2, YMs = Low_Vol, Alias = "Low_Vol")

```




## Caping 

I capped the SWIX to 6 percent and the ALSI to 10 percent (this is how I interpreted the question). 
I couldn.t get the cap exactly right but I think I was on the right path, therefore I left the code in. 

```{r}
#J400
#max_cap <- 0.6 
#min_cap <- 0 

#RebDays %>% filter(date < lubridate::ymd(20211030)) %>% filter(date >= lubridate::ymd(20080102))



#rebal_dates <-data1 %>% 
 # mutate(Year = format(date, "%Y"), Month = format(date, "%b"), Day = format(date, "%a")) %>% 
  #filter(Month %in% c("Mar", "Jun","Sep","Dec")) %>% 
  #filter(Day == c("Thu", "Mon")) %>% 
  #group_by(Year, Month) %>% 
#filter(date == first(date)) %>% 
#pull(date)

#data1$weight <- data1$J400
#data1[is.na(data1)] = 0

#rebalance_col <- 
#data1 %>% 
#filter(date %in% rebal_dates) %>% 
#mutate(RebalTime = format(date, "%Y%B")) %>% 
#group_by(RebalTime) %>% 
 #arrange(date) %>% 
#select(date, Tickers, weight, RebalTime)

#df_Cons <- rebalance_col %>% filter(date==first(date))

#Proportional_Cap_Foo <- function(df_Cons, W_Cap = 0.6){
  
  # Let's require a specific form from the user... Alerting when it does not adhere this form
 # if( !"weight" %in% names(df_Cons)) stop("... for Calc capping to work, provide weight column called 'weight'")
  
 # if( !"date" %in% names(df_Cons)) stop("... for Calc capping to work, provide date column called 'date'")
  
#  if( !"Tickers" %in% names(df_Cons)) stop("... for Calc capping to work, provide id column called 'Tickers'")

  # First identify the cap breachers
 # Breachers <- 
  #  df_Cons %>% filter(weight > W_Cap) %>% pull(Tickers)
  
  # Now keep track of breachers, and add to it to ensure they remain at 10%:
#  if(length(Breachers) > 0) {
    
 #   while( df_Cons %>% filter(weight > W_Cap) %>% nrow() > 0 ) {
      
      
  #    df_Cons <-
        
   #     bind_rows(
          
    #      df_Cons %>% filter(Tickers %in% Breachers) %>% mutate(weight = W_Cap),
          
     #     df_Cons %>% filter(!Tickers %in% Breachers) %>% 
      #      mutate(weight = (weight / sum(weight, na.rm=T)) * (1-length(Breachers)*W_Cap) )
          
       # )
      
  #    Breachers <- c(Breachers, df_Cons %>% filter(weight > W_Cap) %>% pull(Tickers))
      
   # }

  #  if( sum(df_Cons$weight, na.rm=T) > 1.001 | sum(df_Cons$weight, na.rm=T) < 0.999 | max(df_Cons$weight, na.rm = T) > W_Cap) {
      
   #   stop( glue::glue("For the Generic weight trimming function used: the weight trimming causes non unit 
    #  summation of weights for date: {unique(df_Cons$date)}...\n
     # The restriction could be too low or some dates have extreme concentrations...") )
      
  #  }
    
#  } else {
    
#  }
  
 # df_Cons
  
#  }
  

# Now, to map this across all the dates, we can use purrr::map_df as follows:
 #     Capped_df <- 
    
  #  rebalance_col %>% 
    # Split our df into groups (where the groups here are the rebalance dates:
   # group_split(RebalTime) %>% 
    
    # Apply the function Proportional_Cap_Foo to each rebalancing date:
    #map_df(~Proportional_Cap_Foo(., W_Cap = 0.6) ) %>% select(-RebalTime)
  
# Testing this:
#Capped_df %>% pull(weight) %>% max(.)



#rebalance_col$weight[rebalance_col$weight>0.6] = 0.6

#rebalance_col %>% tbl_xts(cols_to_xts = weight, spread_by = Tickers)

```


# Question 3

With the code, I can confirm that there are 92 PCA. This number is a large, however, there are 92 unique tickets in the Top40.This is because every year, some stocks get dropped form the Top40, while others get added. Each of the 92 components explains a percentage of the total variation in the T40 returns. That is, PC1 explains 8 % of the total variance, which is the most out of the 92 PCA's. PC50 to PC92 all explain less than 1 percent of the total variance. The table below plots the individuals components importance. 
```{r}
#Calculating returns

T40[sapply(T40, is.infinite)] <- NA
T40[T40 == 0] <- NA
    
T40 <- T40 %>% na.locf(.,na.rm=T, 5) %>%
    select(date, Tickers, Return, J200) %>%
    mutate(Return = Return*J200) %>%
    select(date, Tickers, Return) %>% 
    group_by(Tickers) %>%
    mutate(Tickers = gsub(" SJ Equity", "", Tickers))

return_mat <- T40 %>% spread(Tickers,Return) 
#colSums(is.na(T40))

impute_missing_returns <- function(return_mat, impute_returns_method = "NONE", Seed = 1234){
  # Make sure we have a date column called date:
  if( !"date" %in% colnames(return_mat) ) stop("No 'date' column provided in return_mat. Try again please.")

  # Note my use of 'any' below...
  # Also note that I 'return' return_mat - which stops the function and returns return_mat. 
  if( impute_returns_method %in% c("NONE", "None", "none") ) {
    if( any(is.na(return_mat)) ) warning("There are missing values in the return matrix.. Consider maybe using impute_returns_method = 'Drawn_Distribution_Own' / 'Drawn_Distribution_Collective'")
    return(return_mat)
  }

  
  if( impute_returns_method  == "Average") {

    return_mat <-
      return_mat %>% gather(Stocks, Returns, -date) %>%
      group_by(date) %>%
      mutate(Avg = mean(Returns, na.rm=T)) %>%
      mutate(Avg = coalesce(Avg, 0)) %>% # date with no returns - set avg to zero
      ungroup() %>%
      mutate(Returns = coalesce(Returns, Avg)) %>% select(-Avg) %>% spread(Stocks, Returns)

    # That is just so much easier when tidy right? See how I gathered and spread again to give back a wide df?
    
  } else

    if( impute_returns_method  == "Drawn_Distribution_Own") {

      set.seed(Seed)
      N <- nrow(return_mat)
      return_mat <-

        left_join(return_mat %>% gather(Stocks, Returns, -date),
                  return_mat %>% gather(Stocks, Returns, -date) %>% group_by(Stocks) %>%
                    do(Dens = density(.$Returns, na.rm=T)) %>%
                    ungroup() %>% group_by(Stocks) %>% # done to avoid warning.
                    do(Random_Draws = sample(.$Dens[[1]]$x, N, replace = TRUE, prob=.$Dens[[1]]$y)),
                  by = "Stocks"
        ) %>%  group_by(Stocks) %>% mutate(Row = row_number()) %>% mutate(Returns = coalesce(Returns, Random_Draws[[1]][Row])) %>%
        select(-Random_Draws, -Row) %>% ungroup() %>% spread(Stocks, Returns)

    } else

      if( impute_returns_method  == "Drawn_Distribution_Collective") {

        set.seed(Seed)
        NAll <- nrow(return_mat %>% gather(Stocks, Returns, -date))

        return_mat <-
          bind_cols(
          return_mat %>% gather(Stocks, Returns, -date),
          return_mat %>% gather(Stocks, Returns, -date) %>%
            do(Dens = density(.$Returns, na.rm=T)) %>%
            do(Random_Draws = sample(.$Dens[[1]]$x, NAll, replace = TRUE, prob=.$Dens[[1]]$y)) %>% unnest(Random_Draws)
          ) %>%
          mutate(Returns = coalesce(Returns, Random_Draws)) %>% select(-Random_Draws) %>% spread(Stocks, Returns)

      } else

        if( impute_returns_method  == "Zero") {
        warning("This is probably not the best idea but who am I to judge....")
          return_mat[is.na(return_mat)] <- 0

        } else
          stop("Please provide a valid impute_returns_method method. Options include:\n'Average', 'Drawn_Distribution_Own', 'Drawn_Distribution_Collective' and 'Zero'.")
}


options(scipen = 999) # Stop the scientific notation of

return_mat <- impute_missing_returns(return_mat, impute_returns_method = "Drawn_Distribution_Collective", Seed = as.numeric(format( Sys.time(), "%Y%d%H%M")))

return_mat_Nodate <- data.matrix(return_mat[, -1])

# Simple Sample covariance and mean:
Sigma <- RiskPortfolios::covEstimation(return_mat_Nodate)
Mu <- RiskPortfolios::meanEstimation(return_mat_Nodate)

#PCA calculations
pca <- prcomp(return_mat_Nodate,center=TRUE, scale.=TRUE)
d <- str(pca)
c <- pca$sdev #extracting standard deviation.
eigenvalues <- pca$sdev^2
b<- pca$rotation
a <- summary(pca)
a
```


```{r}
fviz_screeplot(pca, ncp = 10)
```


As can be seen in the plot below, there is not one variable that makes a significant contribution to the variance. Therefore, you will need to consider all variables of the Top40 to understand how the variance is affected. This graph is not very informative as we don't know which graph belongs to which Ticker. However, adding in rownames makes the plot very messy. 
```{r}

ggbiplot(pca)

```

With the 92 PCA, the information that we gain is not very useful. 

## Cumulative Return 

The graph below gives the cumulative returns for all the unique stocks in the Top40, with different starting dates. Sinces we are analyzing 92 different stocks, the graph is still not clear as to which stocks contribute the most to the top40 portfolio. 
```{r}

gg <-  T40 %>%
    select(date, Tickers, Return, J200) %>%
    mutate(Return = Return*J200) %>%
    select(date, Tickers, Return) %>% 
    arrange(date) %>%
    group_by(Tickers) %>%
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
        mutate(Rets = coalesce(Return, 0)) %>%  
        mutate(CP = cumprod(1 + Rets)) %>% 
        ungroup() %>% ggplot() + 
geom_line(aes(date, CP, color = Tickers)) + 
labs(title = "Cumulative Returns of various Indices", 
    subtitle = "", caption = "Note:\nDistortions emerge as starting dates differ.")
# Level plot
gg

```

# Question 4 

Over the past couple of year, the South African ZAR has been more volatile when compared to the major currencies. There are many factors that affect this volatility. A few of the most prominent include political unrest, central banking policies, economic performance and outlier events. If any of these underpinnings evolve into a dominant market driver, exchange rate volatility facing the ZAR spikes. The result is a destabilisation of the rand and turbulence in the forex valuations of related pairs. 


```{r}

cncy %>% group_by(date) %>%
ggplot() + 
    geom_line(aes(x=date, y=Price, color=Name)) +
    theme_bw() +  
    labs(x = "Dates", y = "Price (relative to USD)", title = "Currency", subtitle = "", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Currency"))

```

Since the graph above is difficult to read, I have decided to only compare the major curries to get a clearer picture. The graph below displays the Euro, the Great British Pound, the Australian Dollar, the Canadian dollar, the Japanese Yen and the South African Rand. 

```{r}
#cncy %>% group_by(Name) %>% pull(Name) %>% unique 
plota <- cncy %>% filter(Name==c("Australia_Cncy_Inv", "Canada_Cncy", "SouthAfrica_Cncy","UK_Cncy_Inv", "EU_Cncy_Inv", "Japan_Cncy"))%>% group_by(date) %>%
ggplot() + 
    geom_line(aes(x=date, y=Price, color=Name)) +
    facet_wrap(~Name, scales = "free_y") +
    theme_bw() +  
    labs(x = "Dates", y = "Price (relative to USD)", title = "Analzying each currency relative to USD", subtitle = "", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Currency"))

plota
```

## Implied Volatility 

Implied volatility is a measure of the expected risk with regards to the underlying for an option. The measure reflects the marketâ€™s view on the likelihood of movements in prices for the underlying, having the tendency to increase when prices decline and thus reflect the riskier picture
 
Implied volatility is commonly used by the market to pre-empt future movements of the underlying. High volatility suggests large price swings, while muted volatility could mean that price fluctuations may be very much contained.

Implied Volatility is important because it  not only incorporates historical information about asset prices but also market participants' expectations, frequently not easily quantifiable, about future events. Looking at the implied volatility graph below, it indicates that South African Rand experiences much higher volatility compared to other currencies. It should be kept in mind that South Africa is an emerging market whereas the other countries curries that are being analyzed are all developed countries. 
```{r}

cncyIV %>% group_by(Name) %>% pull(Name) %>% unique 

plotb <- cncyIV %>% filter(Name==c("Australia_IV", "Canada_IV", "SouthAfrica_IV","UK_IV", "EU_IV", "Japan_IV"))%>% group_by(date) %>%
ggplot() + 
    geom_line(aes(x=date, y=Price, color=Name)) +
    theme_bw() +  
    labs(x = "Dates", y = "Price (relative to USD)", title = "Implied Volatility ", subtitle = "", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Currency"))

plotb

```

## Univariate GARCH 

The graph below shows the USD/ZAR exchange rate. As you can see, the periods of volatility clustering increases with time. The most volatility was experiences during the 2008 Global Financial crisis. This graph gives a good indication to see where the periods of high and low volatility are. 

```{r}
plotc <- cncy %>% filter(Name==c("SouthAfrica_Cncy")) %>% group_by(Name) %>%  mutate(dlogprice = log(Price) - log(lag(Price))) %>%
    filter(date > lubridate::ymd(19900101)) %>%
    ggplot() +
    geom_line(aes(x=date,y=dlogprice), color="red") + theme_bw() +  
    labs(x = "Dates", y = "", title = "USD/ZAR", subtitle = "", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Currency"))


plotc
    
```
In the table below, there are a few summary statistics of the garch model. 

results ='asis'
```{r}

data <-  cncy %>% filter(Name==c( "SouthAfrica_Cncy")) %>% group_by(Name) %>%
    mutate(dlogprice = log(Price) - log(lag(Price))) %>%
    filter(date > lubridate::ymd(19900101)) %>%
    select(date, dlogprice)

garch11 <- 
  
  ugarchspec(
    
    variance.model = list(model = c("sGARCH","gjrGARCH","eGARCH","fGARCH","apARCH")[1], 
                          
    garchOrder = c(1, 1)), 
    
    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
    
    distribution.model = c("norm", "snorm", "std", "sstd", "ged", "sged", "nig", "ghyp", "jsu")[1])

garchfit1 = ugarchfit(spec = garch11,data = data$dlogprice)
slotNames(garchfit1)
names(garchfit1@fit)
names(garchfit1@model)
garchfit1@fit$matcoef 

Table <- xtable(garchfit1@fit$matcoef)

print(Table, type = "latex", comment = FALSE)

```
The Persistence of the garch is 0.998, which is expected as the persistence is the alpha and the beta summed together. The news impact curve also reflects the volatility asymmetry.

```{r}
ni <- newsimpact(z = NULL, garchfit1)
plotd <- plot(ni$zx, ni$zy, ylab = ni$yexpr, xlab = ni$xexpr, type = "l", 
    main = "News Impact Curve")
plotd
```

## Carry trades

The carry trade strategy has been popular around investors since 1980, since on average, the returns are positive. As can be seen in the graph below, the Deutsche Bank G10 harvest Index and the South African ZAR follow eachother, with the ZAR being slightly more volatile. This means that carry trades serve as a good indicater for the USD/ZAR exchange rate. 

```{r}
carry <- cncy_Carry %>% mutate(carry_c = log(Price) - log(lag(Price))) %>%
    mutate(scaledret = (carry_c - mean(carry_c, na.rm = T))) %>% 
filter(date > lubridate::ymd(19900101))  %>%
    select(date, carry_c)
   

data <-  cncy %>% filter(Name==c( "SouthAfrica_Cncy")) %>%
    mutate(SA_ZAR = log(Price) - log(lag(Price))) %>%
    mutate(scaledprice = (SA_ZAR - mean(SA_ZAR, na.rm = T))) %>%
    filter(date > lubridate::ymd(19900101)) %>%
     filter(date >= lubridate::ymd(20000919)) %>%
    select(date, SA_ZAR)


plotg <- left_join(carry, data, by= "date") %>% gather(Name, price, -date) %>% ggplot() + geom_line(aes(x=date,y=price, color=Name)) +
     theme_bw() +  
    labs(x = "Dates", y = "log returns", title = "South African ZAR vs Currency Carry", subtitle = "", caption = "Note:\nOwn Calculations") + guides(col=guide_legend("Currencies"))

plotg

```

# Question 5 

The main idea underlying these portmanteau tests is to identify if there is any dependence structure which is yet unexplained by the fitted model. The MARCH test indicates that all the MV portmanteau tests reject the null of no conditional heteroskedasticity, which supports the use of MVGARCH models. 

```{r}
dat <- msci %>% mutate(MSCI = gsub("MSCI_", "", Name)) %>% 
    select(-Name) %>%
    filter(MSCI == c("RE", "USREIT", "ACWI")) %>% 
    select(date, MSCI, Price) %>%
    spread(MSCI, Price)

    
data <- left_join(msci %>% arrange(date) %>%
                      group_by(Name) %>% 
                      mutate(Price = (Price/lag(Price)) -1) %>%
                     spread(Name, Price), comms %>%
                      group_by(Name) %>% 
                      mutate(Price = Price/lag(Price) -1) %>% 
                      spread(Name, Price), by= "date")

together <- left_join(data, bonds %>%
                      group_by(Name) %>% 
                      mutate(Bond_10Yr = Bond_10Yr/lag(Bond_10Yr) -1) %>%
                          spread(Name, Bond_10Yr), by="date")  %>% select(date, MSCI_Value, MSCI_Growth, US_10Yr, Gold, Oil_Brent, Bcom_Index) %>%
    filter(date > lubridate::ymd(19900101)) %>%
    gather(Name, Price, -date)
  
    
xts_rtn <- together %>% tbl_xts(., cols_to_xts = "Price", spread_by = "Name")
# MarchTest(xts_rtn) # This works fine in the Question part but then as soon as I knit it gives me an error I cant seem to fix
```
The dynamic conditional correlation (DCC) models offer a simple and more parsimonious means of doing MV-vol modeling. The graph below estimates the volatility for each series. 

As seen in the graph, oil was much more volatility than other commodities, equities and bonds in the early 1990's. This is due to Iraqâ€™s invasion of Kuwait and the first Gulf war send prices to a then all-time high of $41.90 a barrel in October but gains are short-lived as U.S.-led forces secure the giant oilfields of Saudi Arabia and tanker lanes in the Gulf. 

After the oil price stabilized in the early 1990's, volatility for bonds, commodities and equities have been very similar and seem to move in the same direction. This can be an indication that different asset classes have increased in their convergences. Holding different asset classes if not diversification. This is simply not putting your egg in the same basket. However, investors should instead put different eggs in negatively correlated basket. The graph below suggest that in the early 1990's, different asset classes may have been efficient enough for diversification, but not anymore. 

For the 2020 time-period, the US 10 year bond has a much sigma then the other asset classes. However, it it important to remember that this is during a nation-wide pandemic, and it is not the norm. Overall, oil prices seem to be the most volatile. 

NOTE: Documents Knits perfectly, however, I did notice that the graph changes drastically as soon you upload onto github. I do not know why this happes. An image of the graph is also uploaded, called "image". 

```{r}
DCCPre <- dccPre(xts_rtn, include.mean = T, p = 0)
names(DCCPre)

Vol <- DCCPre$marVol
colnames(Vol) <- colnames(xts_rtn)
Vol <- 
  data.frame( cbind( date = index(xts_rtn), Vol)) %>% 
  mutate(date = as.Date(date)) %>%  tibble::as_tibble()  

TidyVol <- Vol %>% gather(Name, Sigma, -date)

plot9 <- ggplot(TidyVol) + geom_line(aes(x = date, y = Sigma, colour = Name)) 
plot9

```



```{r}

StdRes <- DCCPre$sresi

pacman::p_load("tidyverse", "tbl2xts", "broom", "rmsfuns", "fmxdat")
detach("package:tidyverse", unload=TRUE)
detach("package:tbl2xts", unload=TRUE)
# DCC <- dccFit(StdRes, type="Engle") # I ran into an error here :"no loop for break/next, jumping to top level" - I tried hard to fix it, but neither me nor google got very far 

pacman::p_load("tidyverse", "tbl2xts", "broom")

```